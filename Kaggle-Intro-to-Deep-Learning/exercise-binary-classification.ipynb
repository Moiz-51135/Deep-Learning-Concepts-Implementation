{"metadata":{"jupytext":{"cell_metadata_filter":"-all","formats":"ipynb"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1480608,"sourceType":"datasetVersion","datasetId":829369}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**This notebook is an exercise in the [Intro to Deep Learning](https://www.kaggle.com/learn/intro-to-deep-learning) course.  You can reference the tutorial at [this link](https://www.kaggle.com/ryanholbrook/binary-classification).**\n\n---\n","metadata":{}},{"cell_type":"markdown","source":"# Introduction #\n\nIn this exercise, you'll build a model to predict hotel cancellations with a binary classifier.","metadata":{}},{"cell_type":"code","source":"# Setup plotting\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-whitegrid')\n# Set Matplotlib defaults\nplt.rc('figure', autolayout=True)\nplt.rc('axes', labelweight='bold', labelsize='large',\n       titleweight='bold', titlesize=18, titlepad=10)\nplt.rc('animation', html='html5')\n\n# Setup feedback system\nfrom learntools.core import binder\nbinder.bind(globals())\nfrom learntools.deep_learning_intro.ex6 import *","metadata":{"lines_to_next_cell":0,"execution":{"iopub.status.busy":"2023-12-15T06:05:50.135110Z","iopub.execute_input":"2023-12-15T06:05:50.135565Z","iopub.status.idle":"2023-12-15T06:05:50.665503Z","shell.execute_reply.started":"2023-12-15T06:05:50.135530Z","shell.execute_reply":"2023-12-15T06:05:50.661641Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_43/3338970720.py:3: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n  plt.style.use('seaborn-whitegrid')\n","output_type":"stream"}]},{"cell_type":"markdown","source":"First, load the *Hotel Cancellations* dataset.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer\n\nhotel = pd.read_csv('../input/dl-course-data/hotel.csv')\n\nX = hotel.copy()\ny = X.pop('is_canceled')\n\nX['arrival_date_month'] = \\\n    X['arrival_date_month'].map(\n        {'January':1, 'February': 2, 'March':3,\n         'April':4, 'May':5, 'June':6, 'July':7,\n         'August':8, 'September':9, 'October':10,\n         'November':11, 'December':12}\n    )\n\nfeatures_num = [\n    \"lead_time\", \"arrival_date_week_number\",\n    \"arrival_date_day_of_month\", \"stays_in_weekend_nights\",\n    \"stays_in_week_nights\", \"adults\", \"children\", \"babies\",\n    \"is_repeated_guest\", \"previous_cancellations\",\n    \"previous_bookings_not_canceled\", \"required_car_parking_spaces\",\n    \"total_of_special_requests\", \"adr\",\n]\nfeatures_cat = [\n    \"hotel\", \"arrival_date_month\", \"meal\",\n    \"market_segment\", \"distribution_channel\",\n    \"reserved_room_type\", \"deposit_type\", \"customer_type\",\n]\n\ntransformer_num = make_pipeline(\n    SimpleImputer(strategy=\"constant\"), # there are a few missing values\n    StandardScaler(),\n)\ntransformer_cat = make_pipeline(\n    SimpleImputer(strategy=\"constant\", fill_value=\"NA\"),\n    OneHotEncoder(handle_unknown='ignore'),\n)\n\npreprocessor = make_column_transformer(\n    (transformer_num, features_num),\n    (transformer_cat, features_cat),\n)\n\n# stratify - make sure classes are evenlly represented across splits\nX_train, X_valid, y_train, y_valid = \\\n    train_test_split(X, y, stratify=y, train_size=0.75)\n\nX_train = preprocessor.fit_transform(X_train)\nX_valid = preprocessor.transform(X_valid)\n\ninput_shape = [X_train.shape[1]]","metadata":{"lines_to_next_cell":2,"execution":{"iopub.status.busy":"2023-12-15T06:06:42.378040Z","iopub.execute_input":"2023-12-15T06:06:42.379220Z","iopub.status.idle":"2023-12-15T06:06:43.664816Z","shell.execute_reply.started":"2023-12-15T06:06:42.379180Z","shell.execute_reply":"2023-12-15T06:06:43.663796Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# 1) Define Model #\n\nThe model we'll use this time will have both batch normalization and dropout layers. To ease reading we've broken the diagram into blocks, but you can define it layer by layer as usual.\n\nDefine a model with an architecture given by this diagram:\n\n<figure style=\"padding: 1em;\">\n<img src=\"https://storage.googleapis.com/kaggle-media/learn/images/V04o59Z.png\" width=\"400\" alt=\"Diagram of network architecture: BatchNorm, Dense, BatchNorm, Dropout, Dense, BatchNorm, Dropout, Dense.\">\n<figcaption style=\"textalign: center; font-style: italic\"><center>Diagram of a binary classifier.</center></figcaption>\n</figure>\n","metadata":{}},{"cell_type":"code","source":"from tensorflow import keras\nfrom tensorflow.keras import layers\n\n# YOUR CODE HERE: define the model given in the diagram\nmodel =  keras.Sequential([\n    layers.BatchNormalization(input_shape=input_shape),\n    layers.Dense(256, activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dropout(0.3),\n    layers.Dense(256, activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dropout(0.3),\n    layers.Dense(1, activation='sigmoid'),\n])\n# Check your answer\nq_1.check()","metadata":{"lines_to_next_cell":2,"execution":{"iopub.status.busy":"2023-12-15T06:10:32.076237Z","iopub.execute_input":"2023-12-15T06:10:32.076711Z","iopub.status.idle":"2023-12-15T06:10:47.434927Z","shell.execute_reply.started":"2023-12-15T06:10:32.076663Z","shell.execute_reply":"2023-12-15T06:10:47.433635Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"outcomeType\": 1, \"valueTowardsCompletion\": 0.3333333333333333, \"interactionType\": 1, \"questionType\": 2, \"questionId\": \"1_Q1\", \"learnToolsVersion\": \"0.3.4\", \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\"}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Correct","text/markdown":"<span style=\"color:#33cc33\">Correct</span>"},"metadata":{}}]},{"cell_type":"markdown","source":"# 2) Add Optimizer, Loss, and Metric #\n\nNow compile the model with the Adam optimizer and binary versions of the cross-entropy loss and accuracy metric.","metadata":{}},{"cell_type":"code","source":"# YOUR CODE HERE\nmodel.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=['binary_accuracy'],\n)\n# Check your answer\nq_2.check()","metadata":{"lines_to_next_cell":0,"execution":{"iopub.status.busy":"2023-12-15T06:15:29.687971Z","iopub.execute_input":"2023-12-15T06:15:29.688356Z","iopub.status.idle":"2023-12-15T06:15:29.713791Z","shell.execute_reply.started":"2023-12-15T06:15:29.688325Z","shell.execute_reply":"2023-12-15T06:15:29.712583Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"outcomeType\": 1, \"valueTowardsCompletion\": 0.3333333333333333, \"interactionType\": 1, \"questionType\": 2, \"questionId\": \"2_Q2\", \"learnToolsVersion\": \"0.3.4\", \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\"}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Correct","text/markdown":"<span style=\"color:#33cc33\">Correct</span>"},"metadata":{}}]},{"cell_type":"code","source":"# Lines below will give you a hint or solution code\n#q_2.hint()\n#q_2.solution()","metadata":{"lines_to_next_cell":0},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, run this cell to train the model and view the learning curves. It may run for around 60 to 70 epochs, which could take a minute or two.","metadata":{}},{"cell_type":"code","source":"early_stopping = keras.callbacks.EarlyStopping(\n    patience=5,\n    min_delta=0.001,\n    restore_best_weights=True,\n)\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_valid, y_valid),\n    batch_size=512,\n    epochs=200,\n    callbacks=[early_stopping],\n)\n\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot(title=\"Cross-entropy\")\nhistory_df.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot(title=\"Accuracy\")","metadata":{"execution":{"iopub.status.busy":"2023-12-15T06:15:37.189506Z","iopub.execute_input":"2023-12-15T06:15:37.189947Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/200\n175/175 [==============================] - 5s 16ms/step - loss: 0.4780 - binary_accuracy: 0.7736 - val_loss: 0.4317 - val_binary_accuracy: 0.7971\nEpoch 2/200\n175/175 [==============================] - 3s 15ms/step - loss: 0.4223 - binary_accuracy: 0.8019 - val_loss: 0.4042 - val_binary_accuracy: 0.8106\nEpoch 3/200\n175/175 [==============================] - 3s 15ms/step - loss: 0.4082 - binary_accuracy: 0.8089 - val_loss: 0.3952 - val_binary_accuracy: 0.8148\nEpoch 4/200\n175/175 [==============================] - 3s 16ms/step - loss: 0.4016 - binary_accuracy: 0.8123 - val_loss: 0.3910 - val_binary_accuracy: 0.8181\nEpoch 5/200\n175/175 [==============================] - 3s 15ms/step - loss: 0.3956 - binary_accuracy: 0.8153 - val_loss: 0.3857 - val_binary_accuracy: 0.8204\nEpoch 6/200\n175/175 [==============================] - 3s 15ms/step - loss: 0.3917 - binary_accuracy: 0.8181 - val_loss: 0.3823 - val_binary_accuracy: 0.8240\nEpoch 7/200\n175/175 [==============================] - 3s 15ms/step - loss: 0.3874 - binary_accuracy: 0.8202 - val_loss: 0.3820 - val_binary_accuracy: 0.8258\nEpoch 8/200\n175/175 [==============================] - 3s 15ms/step - loss: 0.3845 - binary_accuracy: 0.8218 - val_loss: 0.3777 - val_binary_accuracy: 0.8268\nEpoch 9/200\n175/175 [==============================] - 3s 15ms/step - loss: 0.3823 - binary_accuracy: 0.8237 - val_loss: 0.3787 - val_binary_accuracy: 0.8262\nEpoch 10/200\n175/175 [==============================] - 3s 15ms/step - loss: 0.3789 - binary_accuracy: 0.8247 - val_loss: 0.3736 - val_binary_accuracy: 0.8294\nEpoch 11/200\n175/175 [==============================] - 3s 15ms/step - loss: 0.3765 - binary_accuracy: 0.8257 - val_loss: 0.3715 - val_binary_accuracy: 0.8296\nEpoch 12/200\n175/175 [==============================] - 3s 15ms/step - loss: 0.3742 - binary_accuracy: 0.8272 - val_loss: 0.3681 - val_binary_accuracy: 0.8303\nEpoch 13/200\n175/175 [==============================] - 3s 15ms/step - loss: 0.3714 - binary_accuracy: 0.8287 - val_loss: 0.3693 - val_binary_accuracy: 0.8319\nEpoch 14/200\n175/175 [==============================] - 3s 15ms/step - loss: 0.3698 - binary_accuracy: 0.8297 - val_loss: 0.3682 - val_binary_accuracy: 0.8340\nEpoch 15/200\n175/175 [==============================] - 3s 15ms/step - loss: 0.3665 - binary_accuracy: 0.8295 - val_loss: 0.3633 - val_binary_accuracy: 0.8328\nEpoch 16/200\n175/175 [==============================] - 3s 16ms/step - loss: 0.3663 - binary_accuracy: 0.8309 - val_loss: 0.3646 - val_binary_accuracy: 0.8328\nEpoch 17/200\n175/175 [==============================] - 3s 16ms/step - loss: 0.3643 - binary_accuracy: 0.8325 - val_loss: 0.3613 - val_binary_accuracy: 0.8344\nEpoch 18/200\n175/175 [==============================] - 3s 16ms/step - loss: 0.3636 - binary_accuracy: 0.8319 - val_loss: 0.3604 - val_binary_accuracy: 0.8346\nEpoch 19/200\n175/175 [==============================] - 3s 16ms/step - loss: 0.3610 - binary_accuracy: 0.8338 - val_loss: 0.3617 - val_binary_accuracy: 0.8359\nEpoch 20/200\n175/175 [==============================] - 3s 15ms/step - loss: 0.3604 - binary_accuracy: 0.8338 - val_loss: 0.3578 - val_binary_accuracy: 0.8367\nEpoch 21/200\n175/175 [==============================] - 3s 14ms/step - loss: 0.3580 - binary_accuracy: 0.8344 - val_loss: 0.3565 - val_binary_accuracy: 0.8376\nEpoch 22/200\n175/175 [==============================] - 3s 15ms/step - loss: 0.3579 - binary_accuracy: 0.8356 - val_loss: 0.3578 - val_binary_accuracy: 0.8386\nEpoch 23/200\n175/175 [==============================] - 3s 15ms/step - loss: 0.3556 - binary_accuracy: 0.8352 - val_loss: 0.3568 - val_binary_accuracy: 0.8372\nEpoch 24/200\n175/175 [==============================] - 3s 16ms/step - loss: 0.3547 - binary_accuracy: 0.8371 - val_loss: 0.3548 - val_binary_accuracy: 0.8370\nEpoch 25/200\n175/175 [==============================] - 3s 16ms/step - loss: 0.3542 - binary_accuracy: 0.8359 - val_loss: 0.3536 - val_binary_accuracy: 0.8388\nEpoch 26/200\n175/175 [==============================] - 3s 15ms/step - loss: 0.3526 - binary_accuracy: 0.8374 - val_loss: 0.3542 - val_binary_accuracy: 0.8375\nEpoch 27/200\n175/175 [==============================] - 3s 16ms/step - loss: 0.3516 - binary_accuracy: 0.8372 - val_loss: 0.3574 - val_binary_accuracy: 0.8366\nEpoch 28/200\n175/175 [==============================] - 3s 16ms/step - loss: 0.3513 - binary_accuracy: 0.8372 - val_loss: 0.3526 - val_binary_accuracy: 0.8383\nEpoch 29/200\n175/175 [==============================] - 3s 16ms/step - loss: 0.3497 - binary_accuracy: 0.8387 - val_loss: 0.3562 - val_binary_accuracy: 0.8363\nEpoch 30/200\n175/175 [==============================] - 3s 15ms/step - loss: 0.3490 - binary_accuracy: 0.8383 - val_loss: 0.3538 - val_binary_accuracy: 0.8393\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"<Axes: title={'center': 'Accuracy'}>"},"metadata":{}}]},{"cell_type":"markdown","source":"# 3) Train and Evaluate #\n\n\nWhat do you think about the learning curves? Does it look like the model underfit or overfit? Was the cross-entropy loss a good stand-in for accuracy?","metadata":{}},{"cell_type":"code","source":"# View the solution (Run this cell to receive credit!)\nq_3.check()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion #\n\nCongratulations! You've completed Kaggle's *Introduction to Deep Learning* course!\n\nWith your new skills you're ready to take on more advanced applications like computer vision and sentiment classification. What would you like to do next?\n\nWhy not try one of our *Getting Started* competitions?\n\n- Classify images with TPUs in [**Petals to the Metal**](https://www.kaggle.com/c/tpu-getting-started)\n- Create art with GANs in [**I'm Something of a Painter Myself**](https://www.kaggle.com/c/gan-getting-started)\n- Classify Tweets in [**Real or Not? NLP with Disaster Tweets**](https://www.kaggle.com/c/nlp-getting-started)\n- Detect contradiction and entailment in [**Contradictory, My Dear Watson**](https://www.kaggle.com/c/contradictory-my-dear-watson)\n\nUntil next time, Kagglers!","metadata":{}},{"cell_type":"markdown","source":"---\n\n\n\n\n*Have questions or comments? Visit the [course discussion forum](https://www.kaggle.com/learn/intro-to-deep-learning/discussion) to chat with other learners.*","metadata":{}}]}