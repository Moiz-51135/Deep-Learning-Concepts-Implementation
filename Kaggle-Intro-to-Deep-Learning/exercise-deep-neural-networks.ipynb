{"metadata":{"jupytext":{"cell_metadata_filter":"-all","formats":"ipynb"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1480608,"sourceType":"datasetVersion","datasetId":829369}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**This notebook is an exercise in the [Intro to Deep Learning](https://www.kaggle.com/learn/intro-to-deep-learning) course.  You can reference the tutorial at [this link](https://www.kaggle.com/ryanholbrook/deep-neural-networks).**\n\n---\n","metadata":{}},{"cell_type":"markdown","source":"# Introduction #\n\nIn the tutorial, we saw how to build deep neural networks by stacking layers inside a `Sequential` model. By adding an *activation function* after the hidden layers, we gave the network the ability to learn more complex (non-linear) relationships in the data.\n\nIn these exercises, you'll build a neural network with several hidden layers and then explore some activation functions beyond ReLU. Run this next cell to set everything up!","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\n\n# Setup plotting\nimport matplotlib.pyplot as plt\n\nplt.style.use('seaborn-whitegrid')\n# Set Matplotlib defaults\nplt.rc('figure', autolayout=True)\nplt.rc('axes', labelweight='bold', labelsize='large',\n       titleweight='bold', titlesize=18, titlepad=10)\n\n# Setup feedback system\nfrom learntools.core import binder\nbinder.bind(globals())\nfrom learntools.deep_learning_intro.ex2 import *","metadata":{"execution":{"iopub.status.busy":"2023-12-15T05:34:45.986517Z","iopub.execute_input":"2023-12-15T05:34:45.987509Z","iopub.status.idle":"2023-12-15T05:34:59.872323Z","shell.execute_reply.started":"2023-12-15T05:34:45.987457Z","shell.execute_reply":"2023-12-15T05:34:59.871092Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_43/3168024915.py:6: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n  plt.style.use('seaborn-whitegrid')\n","output_type":"stream"}]},{"cell_type":"markdown","source":"In the *Concrete* dataset, your task is to predict the compressive strength of concrete manufactured according to various recipes.\n\nRun the next code cell without changes to load the dataset.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\nconcrete = pd.read_csv('../input/dl-course-data/concrete.csv')\nconcrete.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-15T05:35:02.329845Z","iopub.execute_input":"2023-12-15T05:35:02.330537Z","iopub.status.idle":"2023-12-15T05:35:02.380842Z","shell.execute_reply.started":"2023-12-15T05:35:02.330475Z","shell.execute_reply":"2023-12-15T05:35:02.379706Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"   Cement  BlastFurnaceSlag  FlyAsh  Water  Superplasticizer  CoarseAggregate  \\\n0   540.0               0.0     0.0  162.0               2.5           1040.0   \n1   540.0               0.0     0.0  162.0               2.5           1055.0   \n2   332.5             142.5     0.0  228.0               0.0            932.0   \n3   332.5             142.5     0.0  228.0               0.0            932.0   \n4   198.6             132.4     0.0  192.0               0.0            978.4   \n\n   FineAggregate  Age  CompressiveStrength  \n0          676.0   28                79.99  \n1          676.0   28                61.89  \n2          594.0  270                40.27  \n3          594.0  365                41.05  \n4          825.5  360                44.30  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Cement</th>\n      <th>BlastFurnaceSlag</th>\n      <th>FlyAsh</th>\n      <th>Water</th>\n      <th>Superplasticizer</th>\n      <th>CoarseAggregate</th>\n      <th>FineAggregate</th>\n      <th>Age</th>\n      <th>CompressiveStrength</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>540.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>162.0</td>\n      <td>2.5</td>\n      <td>1040.0</td>\n      <td>676.0</td>\n      <td>28</td>\n      <td>79.99</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>540.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>162.0</td>\n      <td>2.5</td>\n      <td>1055.0</td>\n      <td>676.0</td>\n      <td>28</td>\n      <td>61.89</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>332.5</td>\n      <td>142.5</td>\n      <td>0.0</td>\n      <td>228.0</td>\n      <td>0.0</td>\n      <td>932.0</td>\n      <td>594.0</td>\n      <td>270</td>\n      <td>40.27</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>332.5</td>\n      <td>142.5</td>\n      <td>0.0</td>\n      <td>228.0</td>\n      <td>0.0</td>\n      <td>932.0</td>\n      <td>594.0</td>\n      <td>365</td>\n      <td>41.05</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>198.6</td>\n      <td>132.4</td>\n      <td>0.0</td>\n      <td>192.0</td>\n      <td>0.0</td>\n      <td>978.4</td>\n      <td>825.5</td>\n      <td>360</td>\n      <td>44.30</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# 1) Input Shape #\n\nThe target for this task is the column `'CompressiveStrength'`. The remaining columns are the features we'll use as inputs.\n\nWhat would be the input shape for this dataset?","metadata":{}},{"cell_type":"code","source":"# YOUR CODE HERE\ninput_shape =[8]\n\n# Check your answer\nq_1.check()","metadata":{"lines_to_next_cell":2,"execution":{"iopub.status.busy":"2023-12-15T05:35:17.806332Z","iopub.execute_input":"2023-12-15T05:35:17.807474Z","iopub.status.idle":"2023-12-15T05:35:17.815999Z","shell.execute_reply.started":"2023-12-15T05:35:17.807438Z","shell.execute_reply":"2023-12-15T05:35:17.814327Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"outcomeType\": 1, \"valueTowardsCompletion\": 0.3333333333333333, \"interactionType\": 1, \"questionType\": 2, \"questionId\": \"1_Q1\", \"learnToolsVersion\": \"0.3.4\", \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\"}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Correct","text/markdown":"<span style=\"color:#33cc33\">Correct</span>"},"metadata":{}}]},{"cell_type":"code","source":"# Lines below will give you a hint or solution code\n#q_1.hint()\n#q_1.solution()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2) Define a Model with Hidden Layers #\n\nNow create a model with three hidden layers, each having 512 units and the ReLU activation.  Be sure to include an output layer of one unit and no activation, and also `input_shape` as an argument to the first layer.","metadata":{}},{"cell_type":"code","source":"from tensorflow import keras\nfrom tensorflow.keras import layers\n\n\n# YOUR CODE HERE\nmodel = keras.Sequential([\n    # the hidden ReLU layers\n    layers.Dense(units=512, activation='relu', input_shape=[8]),\n    layers.Dense(units=512, activation='relu'),\n    layers.Dense(units=512, activation='relu'),\n    # the linear output layer \n    layers.Dense(units=1),\n])\n\n# Check your answer\nq_2.check()","metadata":{"lines_to_next_cell":0,"execution":{"iopub.status.busy":"2023-12-15T05:37:44.954609Z","iopub.execute_input":"2023-12-15T05:37:44.954988Z","iopub.status.idle":"2023-12-15T05:37:45.336123Z","shell.execute_reply.started":"2023-12-15T05:37:44.954960Z","shell.execute_reply":"2023-12-15T05:37:45.334441Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"outcomeType\": 1, \"valueTowardsCompletion\": 0.3333333333333333, \"interactionType\": 1, \"questionType\": 2, \"questionId\": \"2_Q2\", \"learnToolsVersion\": \"0.3.4\", \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\"}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Correct","text/markdown":"<span style=\"color:#33cc33\">Correct</span>"},"metadata":{}}]},{"cell_type":"code","source":"# Lines below will give you a hint or solution code\n#q_2.hint()\n#q_2.solution()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3) Activation Layers #\n\nLet's explore activations functions some.\n\nThe usual way of attaching an activation function to a `Dense` layer is to include it as part of the definition with the `activation` argument. Sometimes though you'll want to put some other layer between the `Dense` layer and its activation function. (We'll see an example of this in Lesson 5 with *batch normalization*.) In this case, we can define the activation in its own `Activation` layer, like so:\n\n```\nlayers.Dense(units=8),\nlayers.Activation('relu')\n```\n\nThis is completely equivalent to the ordinary way: `layers.Dense(units=8, activation='relu')`.\n\nRewrite the following model so that each activation is in its own `Activation` layer.","metadata":{}},{"cell_type":"code","source":"### YOUR CODE HERE: rewrite this to use activation layers\nmodel = keras.Sequential([\n    layers.Dense(32,input_shape=[8]),layers.Activation('relu'),\n    layers.Dense(32),\n    layers.Activation('relu'),\n    layers.Dense(1),\n    \n])\n\n# Check your answer\nq_3.check()","metadata":{"lines_to_next_cell":0,"execution":{"iopub.status.busy":"2023-12-15T05:40:27.710450Z","iopub.execute_input":"2023-12-15T05:40:27.710869Z","iopub.status.idle":"2023-12-15T05:40:27.757931Z","shell.execute_reply.started":"2023-12-15T05:40:27.710832Z","shell.execute_reply":"2023-12-15T05:40:27.756641Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"outcomeType\": 1, \"valueTowardsCompletion\": 0.3333333333333333, \"interactionType\": 1, \"questionType\": 2, \"questionId\": \"3_Q3\", \"learnToolsVersion\": \"0.3.4\", \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\"}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Correct","text/markdown":"<span style=\"color:#33cc33\">Correct</span>"},"metadata":{}}]},{"cell_type":"code","source":"# Lines below will give you a hint or solution code\n#q_3.hint()\n#q_3.solution()","metadata":{"execution":{"iopub.status.busy":"2023-12-15T05:40:40.056680Z","iopub.execute_input":"2023-12-15T05:40:40.057627Z","iopub.status.idle":"2023-12-15T05:40:40.063208Z","shell.execute_reply.started":"2023-12-15T05:40:40.057575Z","shell.execute_reply":"2023-12-15T05:40:40.061368Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Optional: Alternatives to ReLU #\n\nThere is a whole family of variants of the `'relu'` activation -- `'elu'`, `'selu'`, and `'swish'`, among others -- all of which you can use in Keras. Sometimes one activation will perform better than another on a given task, so you could consider experimenting with activations as you develop a model. The ReLU activation tends to do well on most problems, so it's a good one to start with.\n\nLet's look at the graphs of some of these. Change the activation from `'relu'` to one of the others named above. Then run the cell to see the graph. (Check out the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/activations) for more ideas.)","metadata":{}},{"cell_type":"code","source":"# YOUR CODE HERE: Change 'relu' to 'elu', 'selu', 'swish'... or something else\nactivation_layer = layers.Activation('r')\n\nx = tf.linspace(-3.0, 3.0, 100)\ny = activation_layer(x) # once created, a layer is callable just like a function\n\nplt.figure(dpi=100)\nplt.plot(x, y)\nplt.xlim(-3, 3)\nplt.xlabel(\"Input\")\nplt.ylabel(\"Output\")\nplt.show()","metadata":{"lines_to_next_cell":0},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Keep Going #\n\nNow move on to Lesson 3 and [**learn how to train neural networks**](https://www.kaggle.com/ryanholbrook/stochastic-gradient-descent) with stochastic gradient descent.","metadata":{}},{"cell_type":"markdown","source":"---\n\n\n\n\n*Have questions or comments? Visit the [course discussion forum](https://www.kaggle.com/learn/intro-to-deep-learning/discussion) to chat with other learners.*","metadata":{}}]}